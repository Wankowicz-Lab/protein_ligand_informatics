{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd3dc55-a3f8-4da1-860e-71e9044867b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP data Conversion complete!\n",
      "The combined dataset has been successfully saved to 'combined_dataset.csv'.\n",
      "OP data subset aligned created\n",
      "Files ending with '5.0_order_param_subset.csv' have been successfully transferred to the separate folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harrw10\\AppData\\Local\\Temp\\ipykernel_22804\\810631744.py:119: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_5_op_subset_df = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a combined subset of OP 5.0 closeres data\n",
      "Created an Apo combined subset of OP 5.0 closeres data\n",
      "Saved Z-score normalized Order Parameter histograms to /Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/combined_full_protein_data/apo_op_histograms_overlay_zscore_normalized\\zscore_normalized_order_parameter_histograms.png\n",
      "Z-score normalized Order Parameter histograms created successfully!\n",
      "OP data analysis complete\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mplcursors\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "#How to convert all .out files from desktop to .csv\n",
    "#Directory containing the .out files (Desktop directory)\n",
    "directory = \"/Users/harrw10/Desktop/op_data\"\n",
    "#Loop through all .out files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".out\"):\n",
    "        # Read the content of the .out file\n",
    "        with open(os.path.join(directory, filename), 'r') as infile:\n",
    "            content = infile.read()\n",
    "        \n",
    "        # Define the output .csv file name\n",
    "        csv_filename = filename.replace(\".out\", \".csv\")\n",
    "        \n",
    "        # Write the content to the .csv file\n",
    "        with open(os.path.join(directory, csv_filename), 'w') as outfile:\n",
    "            outfile.write(content)\n",
    "\n",
    "print(\"OP data Conversion complete!\")\n",
    "\n",
    "#Adding PDB name to a column in each of the op files then create a single combined file\n",
    "# Directory containing the .csv files\n",
    "directory = '/Users/harrw10/Desktop/op_data'\n",
    "\n",
    "#Initialize an empty list to hold the dataframes\n",
    "dataframes = []\n",
    "\n",
    "#List all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the base name without extension and remove '_OP'\n",
    "        base_name, ext = os.path.splitext(filename)\n",
    "        cleaned_base_name = base_name.replace('_OP', '')\n",
    "        \n",
    "        # Read the csv file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        \n",
    "        # Add a new column with the cleaned base name\n",
    "        df['Cleaned Base Name'] = cleaned_base_name\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "#Concatenate all dataframes in the list into a single dataframe\n",
    "combined_op_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "#Save the combined dataframe to a new csv file\n",
    "combined_op_df.to_csv('/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/op_subset_files/op_combined.csv', index=False)\n",
    "\n",
    "print(\"The combined dataset has been successfully saved to 'combined_dataset.csv'.\")\n",
    "\n",
    "#How to combine aspects of the 'Apo' data and OP_subset into a singular file\n",
    "#Define the dataframe for the op data and apo specific proteins\n",
    "combined_op_df = pd.read_csv('/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/op_subset_files/op_combined.csv')\n",
    "df = pd.read_csv('/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/apo_holo_241016.csv')\n",
    "#Define criteria for apo_res structure\n",
    "def apo_df():\n",
    "    filtered_2_df = df[df['Apo'].notna() & (df['Apo'] != '')]\n",
    "# Select only the desired columns\n",
    "    apo_columns = ['Apo']\n",
    "    filtered_2_df = filtered_2_df[apo_columns]\n",
    "    return filtered_2_df\n",
    "apo_data = apo_df()\n",
    "\n",
    "#Create a subset of OP data for only Apo structures\n",
    "apo_op_subset_aligned = combined_op_df[combined_op_df['Cleaned Base Name'] .isin (apo_data['Apo'])]\n",
    "apo_op_subset_aligned.to_csv('/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/combined_full_protein_data/apo_op_subset_aligned.csv',index=False)\n",
    "print(\"OP data subset aligned created\")\n",
    "\n",
    "#How to separate the _5.0_order_param_subset files from the entire data set and transfer them into an individual folder\n",
    "#Define the source and destination directories\n",
    "source_directory = '/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/full_protein-20241018T143402Z-001/full_protein/'\n",
    "destination_directory = '/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt//Stephanie Wankowicz research/5_op_subset_files/'\n",
    "\n",
    "#Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "#List all files in the source directory\n",
    "file_list = os.listdir(source_directory)\n",
    "\n",
    "#Loop through each file and transfer files ending with qFit_sasa.csv to the destination directory\n",
    "for file in file_list:\n",
    "    if file.endswith('_5.0_order_param_subset.csv'):\n",
    "        source_file_path = os.path.join(source_directory, file)\n",
    "        destination_file_path = os.path.join(destination_directory, file)\n",
    "        shutil.move(source_file_path, destination_file_path)\n",
    "\n",
    "print(\"Files ending with '5.0_order_param_subset.csv' have been successfully transferred to the separate folder.\")\n",
    "\n",
    "#How to add the PDB variable to each 5_order_param_subset file then concantenate all dataframes into a single dataframe from the analysis scripts\n",
    "#Directory containing the .csv files\n",
    "directory = '/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt//Stephanie Wankowicz research/5_op_subset_files'\n",
    "\n",
    "#Initialize an empty list to hold the dataframes\n",
    "dataframes = []\n",
    "\n",
    "#List all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the base name without extension and remove \n",
    "        base_name, ext = os.path.splitext(filename)\n",
    "        cleaned_base_name = base_name.replace('_5.0_order_param_subset', '')\n",
    "     #Read the csv file into a dataframe\n",
    "        _5_op_subset_df = pd.read_csv(os.path.join(directory, filename))\n",
    "        \n",
    "        #Add a new column with the cleaned base name\n",
    "        _5_op_subset_df['Cleaned Base Name'] = cleaned_base_name\n",
    "        \n",
    "        #Append the dataframe to the list\n",
    "        dataframes.append(_5_op_subset_df)\n",
    "#Concatenate all dataframes in the list into a single dataframe\n",
    "combined_5_op_subset_df = pd.concat(dataframes, ignore_index=True)  \n",
    "combined_5_op_subset_df.to_csv('/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt//Stephanie Wankowicz research/5_op_subset_files/combined_5_op_subset_df.csv', index=False)\n",
    "print(\"Created a combined subset of OP 5.0 closeres data\")\n",
    "\n",
    "#How to combine aspects of the 'Apo' data and 5_bfactor_subset into a singular file\n",
    "df = pd.read_csv('/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/apo_holo_241016.csv')\n",
    "\n",
    "#Define criteria for apo_res structure\n",
    "def apo_df():\n",
    "    filtered_2_df = df[df['Apo'].notna() & (df['Apo'] != '')]\n",
    "#Select only the desired columns\n",
    "    apo_columns = ['Apo']\n",
    "    filtered_2_df = filtered_2_df[apo_columns]\n",
    "    return filtered_2_df\n",
    "apo_data = apo_df()\n",
    "\n",
    "apo_5_op_subset_aligned = combined_5_op_subset_df[combined_5_op_subset_df['Cleaned Base Name'] .isin (apo_data['Apo'])]\n",
    "apo_5_op_subset_aligned.to_csv('/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/combined_full_protein_data/apo_5_op_subset_aligned.csv',index=False)\n",
    "print(\"Created an Apo combined subset of OP 5.0 closeres data\")\n",
    "\n",
    "#Create the histograms of the OP data and normalize the data using z-score\n",
    "def plot_zscore_normalized_overlaid_b_factor_histograms(df1, df2, label1, label2, output_dir):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    #Calculate normalization parameters from df1 only\n",
    "    mean_df1 = df1['s2calc'].mean()\n",
    "    std_df1 = df1['s2calc'].std()\n",
    "    \n",
    "    #Z-score normalize both datasets using df1's parameters\n",
    "    df1_zscore = (df1['s2calc'] - mean_df1) / std_df1\n",
    "    df2_zscore = (df2['s2calc'] - mean_df1) / std_df1\n",
    "    \n",
    "    #Create the histograms with z-scored data\n",
    "    bins = np.linspace(min(df1_zscore.min(), df2_zscore.min()),\n",
    "                      max(df1_zscore.max(), df2_zscore.max()),\n",
    "                      100)\n",
    "    \n",
    "    n1, bins1, patches1 = ax.hist(df1_zscore, bins=bins, \n",
    "                                 alpha=0.5, label=label1, color='blue', density=True)\n",
    "    n2, bins2, patches2 = ax.hist(df2_zscore, bins=bins, \n",
    "                                 alpha=0.5, label=label2, color='red', density=True)\n",
    "    \n",
    "    ax.set_title('Z-Score Normalized Order Parameter Histograms')\n",
    "    ax.set_xlabel('s2calc (Z-score)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #Add hover annotations\n",
    "    cursor = mplcursors.cursor(patches1 + patches2, hover=True)\n",
    "    \n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        if sel.artist in patches1:\n",
    "            index = patches1.index(sel.artist)\n",
    "            dataset = label1\n",
    "            density = n1[index]\n",
    "        else:\n",
    "            index = patches2.index(sel.artist)\n",
    "            dataset = label2\n",
    "            density = n2[index]\n",
    "        \n",
    "        sel.annotation.set_text(\n",
    "            f'Dataset: {dataset}\\n'\n",
    "            f'Z-score range: {bins[index]:.2f} - {bins[index+1]:.2f}\\n'\n",
    "            f'Density: {density:.4f}'\n",
    "        )\n",
    "    \n",
    "    output_file = os.path.join(output_dir, 'zscore_normalized_order_parameter_histograms.png')\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Saved Z-score normalized Order Parameter histograms to {output_file}\")\n",
    "#Define the file path per dataframe and output directory for histogram\n",
    "def process_datasets(file_path1, file_path2, label1, label2, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df1 = pd.read_csv(file_path1)\n",
    "    df2 = pd.read_csv(file_path2)\n",
    "    plot_zscore_normalized_overlaid_b_factor_histograms(df1, df2, label1, label2, output_dir)\n",
    "    print(\"Z-score normalized Order Parameter histograms created successfully!\")\n",
    "\n",
    "#File paths and settings\n",
    "file_path1 = '/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/combined_full_protein_data/apo_op_subset_aligned.csv'\n",
    "file_path2 = '/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/combined_full_protein_data/apo_5_op_subset_aligned.csv'\n",
    "label1 = 'Apo Order Parameter'\n",
    "label2 = 'Apo Order Parameter 5.0 closeres'\n",
    "output_dir = '/Users/harrw10/OneDrive - Vanderbilt/Documents/Vanderbilt/Stephanie Wankowicz research/combined_full_protein_data/apo_op_histograms_overlay_zscore_normalized'\n",
    "\n",
    "#Process the datasets\n",
    "process_datasets(file_path1, file_path2, label1, label2, output_dir)\n",
    "print(\"OP data analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ad0c5-56eb-4a8f-a3b8-abe9298ef7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
